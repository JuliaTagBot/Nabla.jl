<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Details · Nabla.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/invenia.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Nabla.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><a class="toctext" href="api.html">API</a></li><li><a class="toctext" href="custom.html">Custom Sensitivities</a></li><li class="current"><a class="toctext" href="autodiff.html">Details</a><ul class="internal"><li><a class="toctext" href="#What-is-RMAD?-1">What is RMAD?</a></li><li><a class="toctext" href="#How-does-Nabla-implement-RMAD?-1">How does Nabla implement RMAD?</a></li><li><a class="toctext" href="#A-worked-example-1">A worked example</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href="autodiff.html">Details</a></li></ul><a class="edit-page" href="https://github.com/invenia/Nabla.jl/blob/master/docs/src/pages/autodiff.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Details</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Automatic-Differentiation-1" href="#Automatic-Differentiation-1">Automatic Differentiation</a></h1><p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a>, sometimes abbreviated as &quot;autodiff&quot; or simply &quot;AD,&quot; refers to the process of computing derivatives of arbitrary functions (in the programming sense of the word) in an automated way. There are two primary styles of automatic differentiation: <em>forward mode</em> (FMAD) and <em>reverse mode</em> (RMAD). Nabla&#39;s implementation is based on the latter.</p><h2><a class="nav-anchor" id="What-is-RMAD?-1" href="#What-is-RMAD?-1">What is RMAD?</a></h2><p>A comprehensive introduction to AD is out of the scope of this document. For that, the reader may be interested in books such as <em>Evaluating Derivatives</em> by Griewank and Walther. To give a sense of how Nabla works, we&#39;ll briefly give a high-level overview of RMAD.</p><p>Say you&#39;re evaluating a function <span>$y = f(x)$</span> with the goal of computing the derivative of the output with respect to the input, or, in other words, the <em>sensitivity</em> of the output to changes in the input. Pick an arbitrary intermediate step in the computation of <span>$f$</span>, and suppose it has the form <span>$w = g(u, v)$</span> for some intermediate variables <span>$u$</span> and <span>$v$</span> and function <span>$g$</span>. We denote the derivative of <span>$u$</span> with respect to the input <span>$x$</span> as <span>$\dot{u}$</span>. In FMAD, this is typically the quantity of interest. In RMAD, we want the derivative of the output <span>$y$</span> with respect to (each element of) the intermediate variable <span>$u$</span>, which we&#39;ll denote <span>$\bar{u}$</span>.</p><p><a href="https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf">Giles (2008)</a> shows us that we can compute the sensitivity of <span>$y$</span> to changes in <span>$u$</span> and <span>$v$</span> in reverse mode as</p><div>\[\bar{u} = \left( \frac{\partial g}{\partial u} \right)^{\intercal} \bar{w}, \quad
\bar{v} = \left( \frac{\partial g}{\partial v} \right)^{\intercal} \bar{w}\]</div><p>To arrive at the desired derivative, we start with the identity</p><div>\[\bar{y} = \frac{\partial y}{\partial y} = 1\]</div><p>then work our way backward through the computation of <code>f</code>, at each step computing the sensitivities (e.g. <span>$\bar{w}$</span>) in terms of the sensitivities of the steps which depend on it.</p><p>In Nabla&#39;s implementation of RMAD, we write these intermediate values and the operations that produced them to what&#39;s called a <em>tape</em>. In literature, the tape in this context is sometimes referred to as a &quot;Wengert list.&quot; We do this because, by virtue of working in reverse, we may need to revisit computed values, and we don&#39;t want to have to do each computation again. At the end, we simply sum up the values we&#39;ve stored to the tape.</p><h2><a class="nav-anchor" id="How-does-Nabla-implement-RMAD?-1" href="#How-does-Nabla-implement-RMAD?-1">How does Nabla implement RMAD?</a></h2><p>Take our good friend <span>$f$</span> from before, but now call it <code>f</code>, since now it&#39;s a Julia function containing arbitrary code, among which <code>w = g(u, v)</code> is an intermediate step. With Nabla, we compute <span>$\frac{\partial f}{\partial x}$</span> as <code>∇(f)(x)</code>. Now we&#39;ll take a look inside <code>∇</code> to see how the concepts of RMAD translate to Julia.</p><h3><a class="nav-anchor" id="Computational-graph-1" href="#Computational-graph-1">Computational graph</a></h3><p>Consider the <em>computational graph</em> of <code>f</code>, which you can visualize as a directed acyclic graph where each node is an intermediate step in the computation. In our example, it might look something like</p><pre><code class="language-none">        x        Input
       ╱ ╲
      ╱   ╲
     u     v     Intermediate values computed from x
      ╲   ╱
       ╲ ╱
        w        w = g(u, v)
        │
        y        Output</code></pre><p>where control flow goes from top to bottom.</p><p>To model the computational graph of a function, Nabla uses what it calls <code>Node</code>s, and it stores values to a <code>Tape</code>. <code>Node</code> is an abstract type with subtypes <code>Leaf</code> and <code>Branch</code>. A <code>Leaf</code> is a static quantity that wraps an input value and a tape. As its name suggests, it represents a leaf in the computational graph. A <code>Branch</code> is the result of a function call which has been &quot;intercepted&quot; by Nabla, in the sense that one or more arguments passed to it is a <code>Node</code>. It holds the value of from evaluating the call, as well as information about its position in the computational graph and about the call itself. Functions which should produce <code>Branch</code>es in the computational graph are explicitly extended to do so; this does not happen automatically for each function.</p><h3><a class="nav-anchor" id="Forward-pass-1" href="#Forward-pass-1">Forward pass</a></h3><p>Nabla starts <code>∇(f)(x)</code> off by creating a <code>Tape</code> to hold values and constructing a <code>Leaf</code> that references the tape and the input <code>x</code>. It then performs what&#39;s called the <em>forward pass</em>, where it executes <code>f</code> as usual, walking the computational graph from top to bottom, but with the aforementioned <code>Leaf</code> in place of <code>x</code>. As <code>f</code> is executing, each intercepted function call writes a <code>Branch</code> to the tape. The end result is a fully populated tape that will be used in the <em>reverse pass</em>.</p><h3><a class="nav-anchor" id="Reverse-pass-1" href="#Reverse-pass-1">Reverse pass</a></h3><p>During the reverse pass, we make another pass over the computational graph of <span>$f$</span>, but instead of going from top to bottom, we&#39;re working our way from bottom to top.</p><p>We start with an empty tape the same length as the one populated in the forward pass, but with a 1 in the last place, corresponding to the identity <span>$\bar{y} = 1$</span>. We then traverse the forward tape, compute the sensitivity for each <code>Branch</code>, and store it in the corresponding position in the reverse tape. This process happens in an internal function called <code>propagate</code>.</p><p>The computational graph in question may not be linear, which means we may end up needing to &quot;update&quot; a value we&#39;ve already stored to the tape. By the chain rule, this is just a simple sum of the existing value on the tape with the new value.</p><h3><a class="nav-anchor" id="Computing-derivatives-1" href="#Computing-derivatives-1">Computing derivatives</a></h3><p>As we&#39;re propagating sensitivities up the graph during the reverse pass, we&#39;re calling <code>∇</code> on each intermediate computation. In the case of <code>f</code>, this means that when computing the sensitivity <code>w̄</code> for the intermediate variable <code>w</code>, we will call <code>∇</code> on <code>g</code>.</p><p>This is where the real power of Nabla comes into play. In Julia, every function has its own type, which permits defining methods that dispatch on the particular function passed to it. <code>∇</code> makes heavy use of this; each custom sensitivity is implemented as a method of <code>∇</code>. If no specific method for a particular function has been defined, Nabla enters the function and records its operations as though they were part of the outer computation.</p><p>In our example, if we have no method <code>∇</code> specialized on <code>g</code>, calling <code>∇</code> on <code>g</code> during the reverse pass will look inside of <code>g</code> and write each individual operation it does to the tape. If <code>g</code> is large and does a lot of stuff, this can end up writing a lot to the tape. Given that the tape holds the value of each step, that means it could end up using a lot of memory.</p><p>But if we know how to compute the requisite sensitivities already, we can define a method with the signature</p><pre><code class="language-julia">∇(::typeof(g), ::Type{Arg{i}}, _, y, ȳ, u, v)</code></pre><p>where:</p><ul><li><code>i</code> denotes the <code>i</code>th argument to <code>g</code> (i.e. 1 for <code>u</code> or 2 for <code>v</code>) which dictates whether we&#39;re computing e.g. <code>ū</code> (1) or <code>v̄</code> (2),</li><li><code>_</code> is a placeholder that can be safely ignored for our purposes,</li><li><code>y</code> is the value of <code>g(u, v)</code> computed during the forward pass,</li><li><code>ȳ</code> is the &quot;incoming&quot; sensitivity (i.e. the sensitivity propagated to the current call by the call in the previous node of the graph), and</li><li><code>u</code> and <code>v</code> are the arguments to <code>g</code>.</li></ul><p>We can also tell Nabla how to update an existing tape value with the computed sensitivity by defining a second method of the form</p><pre><code class="language-julia">∇(x̄, ::typeof(g), ::Type{Arg{i}}, _, y, ȳ, u, v)</code></pre><p>which effectively computes</p><pre><code class="language-julia">x̄ += ∇(g, Arg{i}, _, y, ȳ, u, v)</code></pre><p>Absent a specific method of that form, the <code>+=</code> definition above is used literally.</p><h2><a class="nav-anchor" id="A-worked-example-1" href="#A-worked-example-1">A worked example</a></h2><p>So far we&#39;ve seen a bird&#39;s eye view of how Nabla works, so to solidify it a bit, let&#39;s work through a specific example.</p><p>Let&#39;s say we want to compute the derivative of</p><div>\[z = xy + \sin(x)\]</div><p>where <code>x</code> and <code>y</code> (and by extension <code>z</code>) are scalars. The computational graph looks like</p><pre><code class="language-none">      x      y
      │╲     │
      │ ╲    │
      │  ╲   │
      │   ╲  │
  sin(x)   x*y
       ╲   ╱
        ╲ ╱
         z</code></pre><p>A bit of basic calculus tells us that</p><div>\[\frac{\partial z}{\partial x} = \cos(x) + y, \quad
\frac{\partial z}{\partial y} = x\]</div><p>which means that, using the result noted earlier, our reverse mode sensitivities should be</p><div>\[\bar{x} = (\cos(x) + y) \bar{z}, \quad
\bar{y} = x \bar{z}\]</div><p>Since we aren&#39;t dealing with matrices in this case, we can leave off the transpose of the partials.</p><h3><a class="nav-anchor" id="Going-through-manually-1" href="#Going-through-manually-1">Going through manually</a></h3><p>Let&#39;s try defining a tape and doing the forward pass ourselves:</p><pre><code class="language-julia-repl">julia&gt; using Nabla

julia&gt; t = Tape()  # our forward tape
Tape with 0 elements

julia&gt; x = Leaf(t, randn())
Leaf{Float64} 0.6791074260357777

julia&gt; y = Leaf(t, randn())
Leaf{Float64} 0.8284134829000359

julia&gt; z = x*y + sin(x)
Branch{Float64} 1.1906804805361544 f=+</code></pre><p>We can now examine the populated tape <code>t</code> to get a glimpse into what Nabla saw as it walked the tree for the forward pass:</p><pre><code class="language-julia-repl">julia&gt; t
Tape with 5 elements:
  [1]: Leaf{Float64} 0.6791074260357777
  [2]: Leaf{Float64} 0.8284134829000359
  [3]: Branch{Float64} 0.5625817480655771 f=*
  [4]: Branch{Float64} 0.6280987324705773 f=sin
  [5]: Branch{Float64} 1.1906804805361544 f=+</code></pre><p>We can write this out as a series of steps that correspond to the positions in the tape:</p><ol><li><span>$w_1 = x$</span></li><li><span>$w_2 = y$</span></li><li><span>$w_3 = w_1 w_2$</span></li><li><span>$w_4 = \sin(w_1)$</span></li><li><span>$w_5 = w_3 + w_4$</span></li></ol><p>Now let&#39;s do the reverse pass. Here we&#39;re going to be calling some functions that are called internally in Nabla but aren&#39;t intended to be user-facing; they&#39;re used here for the sake of explanation. We start by constructing a reverse tape that will be populated in this pass. The second argument here corresponds to our &quot;seed&quot; value, which is typically 1, per the identity <span>$\bar{z} = 1$</span> noted earlier.</p><pre><code class="language-julia-repl">julia&gt; z̄ = 1.0
1.0

julia&gt; rt = Nabla.reverse_tape(z, z̄)
Tape with 5 elements:
  [1]: #undef
  [2]: #undef
  [3]: #undef
  [4]: #undef
  [5]: 1.0</code></pre><p>And now we use our forward and reverse tapes to do the reverse pass, propagating the sensitivities up the computational tree:</p><pre><code class="language-julia-repl">julia&gt; Nabla.propagate(t, rt)
Tape with 5 elements:
  [1]: 1.6065471361170487
  [2]: 0.6791074260357777
  [3]: 1.0
  [4]: 1.0
  [5]: 1.0</code></pre><p>Revisiting the list of steps, applying the reverse mode sensitivity definition to each, we get a new list, which reads from bottom to top:</p><ol><li><span>$\bar{w}_1 =  \frac{\partial w_4}{\partial w_1} \bar{w}_4 + \frac{\partial w_3}{\partial w_1} \bar{w}_3 =  \cos(w_1) \bar{w}_4 + w_2 \bar{w}_3 =  \cos(w_1) + w_2 =  \cos(x) + y$</span></li><li><span>$\bar{w}_2 = \frac{\partial w_3}{\partial w_2} \bar{w}_3 = w_1 \bar{w}_3 = x$</span></li><li><span>$\bar{w}_3 = \frac{\partial w_5}{\partial w_3} \bar{w}_5 = 1$</span></li><li><span>$\bar{w}_4 = \frac{\partial w_5}{\partial w_4} \bar{w}_5 = 1$</span></li><li><span>$\bar{w}_5 = \bar{z} = 1$</span></li></ol><p>This leaves us with</p><div>\[\bar{x} = \cos(x) + y = 1.60655, \quad \bar{y} = x = 0.67911\]</div><p>which looks familiar! Those are the partial derivatives derived earlier (with <span>$\bar{z} = 1$</span>), evaluated at our values of <span>$x$</span> and <span>$y$</span>.</p><p>We can check our work against what Nabla gives us without going through all of this manually:</p><pre><code class="language-julia-repl">julia&gt; ∇((x, y) -&gt; x*y + sin(x))(0.6791074260357777, 0.8284134829000359)
(1.6065471361170487, 0.6791074260357777)</code></pre><h3><a class="nav-anchor" id="Defining-a-custom-sensitivity-1" href="#Defining-a-custom-sensitivity-1">Defining a custom sensitivity</a></h3><p>Generally speaking, you won&#39;t need to go through these steps. Instead, if you have expressions for the partial derivatives, as we did above, you can define a custom sensitivity.</p><p>Start by defining the function:</p><pre><code class="language-julia-repl">julia&gt; f(x::Real, y::Real) = x*y + sin(x)
f (generic function with 1 method)</code></pre><p>Now we need to tell <code>f</code> that we want Nabla to be able to &quot;intercept&quot; it in order to produce an explicit branch on <code>f</code> in the overall computational graph. That means that our computational graph from Nabla&#39;s perspective is simply</p><pre><code class="language-none">    x     y
     ╲   ╱
    f(x,y)
       │
       z</code></pre><p>We do this with the <code>@explicit_intercepts</code> macro, which defines methods for <code>f</code> that accept <code>Node</code> arguments.</p><pre><code class="language-julia-repl">julia&gt; @explicit_intercepts f Tuple{Real, Real}
f (generic function with 4 methods)

julia&gt; methods(f)
# 4 methods for generic function &quot;f&quot;:
[1] f(x::Real, y::Real) in Main at REPL[18]:1
[2] f(363::Real, 364::Node{#s1} where #s1&lt;:Real) in Main at REPL[19]:1
[3] f(365::Node{#s2} where #s2&lt;:Real, 366::Real) in Main at REPL[19]:1
[4] f(367::Node{#s3} where #s3&lt;:Real, 368::Node{#s4} where #s4&lt;:Real) in Main at REPL[19]:1</code></pre><p>Now we define our sensitivities for <code>f</code> as methods of <code>∇</code>:</p><pre><code class="language-julia-repl">julia&gt; Nabla.∇(::typeof(f), ::Type{Arg{1}}, _, z, z̄, x, y) = (cos(x) + y)*z̄  # x̄

julia&gt; Nabla.∇(::typeof(f), ::Type{Arg{2}}, _, z, z̄, x, y) = x*z̄  # ȳ</code></pre><p>And finally, we can call <code>∇</code> on <code>f</code> to compute the partial derivatives:</p><pre><code class="language-julia-repl">julia&gt; ∇(f)(0.6791074260357777, 0.8284134829000359)
(1.6065471361170487, 0.6791074260357777)</code></pre><p>This gives us the same result at which we arrived when doing things manually.</p><footer><hr/><a class="previous" href="custom.html"><span class="direction">Previous</span><span class="title">Custom Sensitivities</span></a></footer></article></body></html>
